<!DOCTYPE HTML PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<title>Siddhartha Chaudhuri: News</title>
<link rel="stylesheet" type="text/css" href="sidch.css">

</head>

<body>

<div id="Content">

<h1>News</h1>

<ul>

<li>
  <i><font color="#888888">[Jan 2025]</font></i>&nbsp;&nbsp; Our <a href=index.html#tpami2025_metric>paper</a> on unsupervised, temporally-consistent reconstruction of a time-varying shape (e.g. point clouds from motion-capture), expanding and updating our <a href=index.html#iccv2021_metric>ICCV 2021 paper</a>, was accepted to <a href=https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34>TPAMI</a> after spending 3 years (!) in a single review cycle. Fortunately, it is not yet out-of-date :).
</li>

<li>
  <i><font color="#888888">[Jul 2024]</font></i>&nbsp;&nbsp; Two papers accepted to <a href=https://eccv2024.ecva.net/Conferences/2024/>ECCV 2024</a>, one on <a href=index.html#eccv2024_decollage>local detailization of 3D shapes</a>, and the other on <a href=index.html#eccv2024_trj>rig-free motion transfer</a>.
</li>

<li>
  <i><font color="#888888">[May 2024]</font></i>&nbsp;&nbsp; Our <a href=index.html#icml2024>paper</a> on a neurosymbolic system for visual program inference, with both generative and discriminative applications, was accepted to <a href=https://icml.cc/Conferences/2024/>ICML 2024</a>.
</li>

<li>
  <i><font color="#888888">[Apr 2024]</font></i>&nbsp;&nbsp; Our <a href=index.html#siggraph2024>paper</a> on generating 3D shapes via intermediate synthesis of shape skeletons was accepted to <a href=https://s2024.siggraph.org/>SIGGRAPH 2024</a> (conference track).
</li>

<li>
  <i><font color="#888888">[Feb 2024]</font></i>&nbsp;&nbsp; I will serve as an <a href=https://www.computer.org/csdl/journal/tg/about/107398>Associate Editor</a> of <a href=https://www.computer.org/csdl/journal/tg>IEEE Trans. Visualization and Computer Graphics (TVCG)</a>.
</li>

<li>
  <i><font color="#888888">[Sep 2023]</font></i>&nbsp;&nbsp; I will serve on the <a href=https://s2024.siggraph.org>SIGGRAPH 2024</a> Technical Papers Committee and the <a href=https://eg2024.cyens.org.cy/>Eurographics 2024</a> International Program Committee.
</li>

<li>
  <i><font color="#888888">[Aug 2023]</font></i>&nbsp;&nbsp; Our <a href=index.html#sigasia2023>paper</a> on leveraging generative models to explore high-quality mesh deformations was accepted to <a href=https://asia.siggraph.org/2023/>SIGGRAPH Asia 2023</a> (conference track).
</li>

<li>
  <i><font color="#888888">[Mar 2023]</font></i>&nbsp;&nbsp; Our <a href=index.html#siggraph2023>paper</a> on fast complementary dynamics -- real-time secondary motions orthogonal to rig-induced deformation -- was accepted to <a href=https://s2023.siggraph.org>SIGGRAPH 2023</a> (journal track).
</li>

<li>
  <i><font color="#888888">[Feb 2023]</font></i>&nbsp;&nbsp; Our <a href=index.html#cvpr2023>paper</a> on unsupervised shape reconstruction by part retrieval and assembly was accepted to <a href=https://cvpr2023.thecvf.com>CVPR 2023</a>.
</li>

<li>
  <i><font color="#888888">[Jan 2023]</font></i>&nbsp;&nbsp; I'm an awardee of the <a href=https://insaindia.res.in/scroll_news_pdf/Recipients-INSA-Young-Scientist-Award-2022.pdf>Indian National Science Academy's Medal for Young Scientists, 2022</a>. I'm grateful to the students, collaborators, and mentors who made our research possible.
</li>

<li>
  <i><font color="#888888">[Jan 2023]</font></i>&nbsp;&nbsp; I will serve as an <a href=https://onlinelibrary.wiley.com/page/journal/14678659/homepage/editorialboard.html>Associate Editor</a> of <a href=https://onlinelibrary.wiley.com/journal/14678659>Computer Graphics Forum</a> (2023-2026).
</li>

<li>
  <i><font color="#888888">[Jul 2022]</font></i>&nbsp;&nbsp; I will serve on the <a href=https://s2023.siggraph.org>SIGGRAPH 2023</a> Technical Papers Committee.
</li>

<li>
  <i><font color="#888888">[Jul 2022]</font></i>&nbsp;&nbsp; I will teach a session on geometric deep learning at the <a href=https://shapemodelling.iiitd.edu.in/>ACM India Summer School on Shape Modelling</a>.
</li>

<li>
  <i><font color="#888888">[Jul 2022]</font></i>&nbsp;&nbsp; Two papers accepted to ECCV 2022, the <a href=index.html#eccv2022_patchrd>first</a> on a patch-based approach for detail-preserving 3D shape completion, and the <a href=index.html#eccv2022_slotmachine>second</a> on connection-aware reasoning for automatically assembling shapes from parts.
</li>

<li>
  <i><font color="#888888">[Jun 2022]</font></i>&nbsp;&nbsp; I participated in a seminar on <a href=https://cipher.ashoka.edu.in/event/facial-recognition-technology/>"Constitutional Rights and Facial Recognition Technology"</a> organized by <a href=https://www.dsci.in/>DSCI</a> and <a href=https://www.ashoka.edu.in/>Ashoka University</a>. I discussed the underlying technologies, and the dangers of unregulated deployment (and whether they should be deployed at all). Recording at first link.
</li>

<li>
  <i><font color="#888888">[Apr 2022]</font></i>&nbsp;&nbsp; Our <a href=index.html#siggraph2022>paper</a> on neural Jacobian fields for shape mapping was accepted to <a href=https://s2022.siggraph.org>SIGGRAPH 2022</a> (journal track).
</li>

<li>
  <i><font color="#888888">[Mar 2022]</font></i>&nbsp;&nbsp; Our <a href=index.html#cvpr2022>paper</a> on augmenting geometric shape spaces by alternating between generative model training and energy-guided latent space exploration was accepted to <a href=https://cvpr2022.thecvf.com>CVPR 2022</a>.
</li>

<li>
  <i><font color="#888888">[Jan 2022]</font></i>&nbsp;&nbsp; I will serve on the Program Committees of the <a href=https://ijcai-22.org/calls-creativity/>IJCAI-ECAI 2022 Special Track on AI, the Arts and Creativity</a>, and <a href=https://smiconf.github.io/2022/>SMI 2022</a>.
</li>

<li>
  <i><font color="#888888">[Aug 2021]</font></i>&nbsp;&nbsp; Our <a href=index.html#tvcg2021>paper</a> on optimizing the geometry of body-supporting surfaces (e.g. seats of chairs or soles of shoes) to improve comfort, using a novel ergonomic objective based on differentiable physical simulation of a virtual human, was accepted to TVCG.
</li>

<li>
  <i><font color="#888888">[Jul 2021]</font></i>&nbsp;&nbsp; Two papers accepted to <a href=http://iccv2021.thecvf.com>ICCV 2021</a>, one on <a href=index.html#iccv2021_buildingnet>a new dataset and learning method for labeling parts of 3D buildings</a> as an oral, and one on <a href=index.html#iccv2021_metric>temporally-consistent atlas-based surface reconstruction</a> as a poster.
</li>

<li>
  <i><font color="#888888">[Jun 2021]</font></i>&nbsp;&nbsp; Our <a href=index.html#ecmlpkdd2021>paper</a> on a batch selection policy for active metric learning, that uses submodular optimization of joint entropy to balance informativeness and diversity, was accepted to <a href=https://2021.ecmlpkdd.org>ECML PKDD 2021</a>.
</li>

<li>
  <i><font color="#888888">[Jun 2021]</font></i>&nbsp;&nbsp; I will deliver a keynote talk at <a href=https://smi2021.github.io/#program>Shape Modeling International (SMI) 2021</a> in November.
</li>

<li>
  <i><font color="#888888">[Jun 2021]</font></i>&nbsp;&nbsp; Our CVPR workshop on <a href=https://learn3dg.github.io/>Learning to Generate 3D Shapes and Scenes</a> will take place on Friday Jun 25.
</li>

<li>
  <i><font color="#888888">[Apr 2021]</font></i>&nbsp;&nbsp; Recordings of two recent talks are available online: <a href=https://youtu.be/lStGrpg-Knc>3DGV Seminar</a> and <a href=https://youtu.be/Pi7W6XrFtMs>Toronto Geometry Colloquium</a>.
</li>

<li>
  <i><font color="#888888">[Apr 2021]</font></i>&nbsp;&nbsp; I am co-organizing a workshop on <a href=https://geometry.stanford.edu/struco3d/>Structural and Compositional Learning on 3D Data</a> at ICCV 2021.
</li>

<li>
  <i><font color="#888888">[Mar 2021]</font></i>&nbsp;&nbsp; Two papers accepted to <a href=http://cvpr2021.thecvf.com>CVPR 2021</a>, one on <a href=index.html#cvpr2021_decorgan>conditional detailization of shapes</a> as an oral, and one on <a href=index.html#cvpr2021_joint>joint retrieval and deformation</a> as a poster.
</li>

<li>
  <i><font color="#888888">[Mar 2021]</font></i>&nbsp;&nbsp; I will deliver talks on 3D geometry processing at the <a href=https://3dgv.github.io>3DGV Seminar</a> (April 14th India/April 13th US) and the <a href=https://toronto-geometry-colloquium.github.io>Toronto Geometry Colloquium</a> (April 21st).
</li>

<li>
  <i><font color="#888888">[Feb 2021]</font></i>&nbsp;&nbsp; I will serve on the Program Committee of <a href=https://sgp2020.sites.uu.nl/>SGP 2021</a>.
</li>

<li>
  <i><font color="#888888">[Oct 2020]</font></i>&nbsp;&nbsp; Two papers accepted to <a href=http://3dv2020.dgcv.nii.ac.jp>3DV 2020</a>, one on <a href=index.html#3dv2020_coalesce>component assembly with joint synthesis</a> as an oral, and one on <a href=index.html#3dv2020_motion>motion annotation programs</a> as a poster.
</li>

<li>
  <i><font color="#888888">[Jul 2020]</font></i>&nbsp;&nbsp; Our <a href=index.html#eccv2020>paper</a> on approximating 3D point clouds with collections of parametric primitives including b-spline patches was accepted to <a href=https://eccv2020.eu>ECCV 2020</a>.
</li>

<li>
  <i><font color="#888888">[Jun 2020]</font></i>&nbsp;&nbsp; The <a href=https://youtu.be/CZSElr4692s>recording of my (virtual) talk</a> on reducing supervision for shape segmentation, at the workshop on <a href=https://sites.google.com/view/geometry-learning-foundation/>Deep Learning Foundations of Geometric Shape Modeling and Reconstruction</a> at CVPR 2020, is now online.
</li>

<li>
  <i><font color="#888888">[Apr 2020]</font></i>&nbsp;&nbsp; Our <a href=index.html#ijcai2020>paper</a> on decorrelating batches of triplets for active metric learning was accepted to <a href=https://www.ijcai20.org>IJCAI-PRICAI 2020</a>.
</li>

<li>
  <i><font color="#888888">[Mar 2020]</font></i>&nbsp;&nbsp; Our <a href=index.html#siggraph2020>paper</a> on neural subdivision was accepted to <a href=https://s2020.siggraph.org>SIGGRAPH 2020</a>.
</li>

<li>
  <i><font color="#888888">[Feb 2020]</font></i>&nbsp;&nbsp; <a href=index.html#cvpr2020_cages>Three papers</a> accepted to <a href=http://cvpr2020.thecvf.com>CVPR 2020</a>, including two for oral presentation.
</li>

<li>
  <i><font color="#888888">[Feb 2020]</font></i>&nbsp;&nbsp; I will serve on the Program Committee of <a href=https://sgp2020.sites.uu.nl/>SGP 2020</a>.
</li>

<li>
  <i><font color="#888888">[Dec 2019]</font></i>&nbsp;&nbsp; Our <a href=index.html#egstar2020>state-of-the-art report</a> (STAR) on "Learning Generative Models of 3D Structures" was conditionally accepted to Eurographics 2020.
</li>

<li>
  <i><font color="#888888">[Dec 2019]</font></i>&nbsp;&nbsp; I gave a <a href=http://ncvpripg.kletech.ac.in/keynote.html>keynote talk</a> on "Learning to Generate 3D Structures" at NCVPRIPG 2019. My <a href=docs/ncvpripg2019_keynote.pdf>slides</a> are now online.
</li>

<li>
  <i><font color="#888888">[Dec 2019]</font></i>&nbsp;&nbsp; I am co-organizing a <a href=https://learn3dgen.github.io/>workshop</a> on "Learning 3D Generative Models" at <a href=http://cvpr2020.thecvf.com>CVPR 2020</a>. Please submit your papers! We also have an exciting lineup of invited speakers.
</li>

<li>
  <i><font color="#888888">[Dec 2019]</font></i>&nbsp;&nbsp; I will serve on the Program Committees of <a href=https://www.ijcai20.org/>IJCAI 2020</a> and <a href=https://graphicsinterface.org/conference/2020/>Graphics Interface 2020</a>.
</li>

<li>
  <i><font color="#888888">[Oct 2019]</font></i>&nbsp;&nbsp; <a href=https://github.com/sidch/Thea>Thea</a> documentation is now <a href=https://sidch.github.io/Thea>available online</a> via GitHub Pages + Travis.
</li>

<li>
  <i><font color="#888888">[Jul 2019]</font></i>&nbsp;&nbsp; Our <a href=index.html#iccv2019>paper</a> on a new neural network architecture for unsupervised, weakly supervised and one-shot 3D shape segmentation was accepted to ICCV 2019.
</li>

<li>
  <i><font color="#888888">[Jun 2019]</font></i>&nbsp;&nbsp; I will give a talk on deep recursive models for scene synthesis at the <a href=https://3dscenegen.github.io/>3D Scene Generation workshop</a> at CVPR 2019.
</li>

<li>
  <i><font color="#888888">[May 2019]</font></i>&nbsp;&nbsp; Our <a href=index.html#whc2019>paper</a> on deep metric learning using both distinguishable (orderable) and indistinguishable (unorderable) triplets, to model the perceptual similarity of haptic textures, was accepted to WHC 2019.
</li>

<li>
  <i><font color="#888888">[Feb 2019]</font></i>&nbsp;&nbsp; Our <a href=index.html#cvpr2019>paper</a> on a unified deep representation of 3D shapes was accepted to CVPR 2019.
</li>

<li>
  <i><font color="#888888">[Feb 2019]</font></i>&nbsp;&nbsp; I will co-teach a <a href=https://3dstructgen.github.io/>tutorial</a> on learning generative models for 3D structures at Eurographics 2019.
</li>

<li>
  <i><font color="#888888">[Feb 2019]</font></i>&nbsp;&nbsp; I will give a talk on deep recursive models for shape synthesis at the inaugural <a href=https://aiworkshopindia.github.io>NVIDIA AI workshop</a>.
</li>

<li>
  <i><font color="#888888">[Feb 2019]</font></i>&nbsp;&nbsp; I will serve on the Program Committees of <a href=https://sgp2019.di.unimi.it>SGP 2019</a>, <a href=https://www.ijcai19.org/>IJCAI 2019</a>, and <a href=http://irc.cs.sdu.edu.cn/cadcg2019>CAD/Graphics 2019</a>.
</li>

<li>
  <i><font color="#888888">[Oct 2018]</font></i>&nbsp;&nbsp; I received the IIT Bombay Early Research Achiever Award (one of four recipients) for 2017.
</li>

<li>
  <i><font color="#888888">[Oct 2018]</font></i>&nbsp;&nbsp; I will serve on the Program Committee of <a href=http://graphicsinterface.org/conference/2019/>Graphics Interface 2019</a>.
</li>

<li>
  <i><font color="#888888">[Sep 2018]</font></i>&nbsp;&nbsp; Our <a href=index.html#tog2019_grains>paper</a> on a recursive neural network architecture for indoor scene generation was conditionally accepted (with minor revisions) to TOG.
</li>

<li>
  <i><font color="#888888">[Aug 2018]</font></i>&nbsp;&nbsp; Our <a href=index.html#sigasia2018>paper</a> on shape composition using recursive substructure priors was accepted to SIGGRAPH Asia 2018.
</li>

<li>
  <i><font color="#888888">[Jul 2018]</font></i>&nbsp;&nbsp; We posted a <a href=index.html#tog2019_grains>new preprint</a> on a recursive neural network architecture for indoor scene generation.
</li>

<li>
  <i><font color="#888888">[Jul 2018]</font></i>&nbsp;&nbsp; Our <a href=index.html#3dv2018>paper</a> on learning local geometric descriptors that are sensitive to the physical material of the object was accepted to 3DV 2018.
</li>

<li>
  <i><font color="#888888">[Apr 2018]</font></i>&nbsp;&nbsp; My colleagues at IITB released a <a href=https://www.cse.iitb.ac.in/identity/docs/aadhaar-whitepaper.pdf>whitepaper</a> analyzing the shortcomings of the current Aadhaar national identity scheme, and proposing technical and policy guidelines for a sounder approach. We have now posted a short <a href=https://www.cse.iitb.ac.in/identity/docs/aadhaar-whitepaper-overview.html>summary</a> of the whitepaper. All writings on the subject are collected <a href=https://www.cse.iitb.ac.in/identity>here</a>.
</li>

<li>
  <i><font color="#888888">[Feb 2018]</font></i>&nbsp;&nbsp; Our <a href=index.html#cvpr2018>paper</a> on a novel neural network architecture for weakly-supervised shape segmentation was accepted to CVPR 2018.
</li>

<li>
  <i><font color="#888888">[Jan 2018]</font></i>&nbsp;&nbsp; Our <a href=index.html#iclr2018>paper</a> on training deep networks for domain generalization was accepted to ICLR 2018.
</li>

<li>
  <i><font color="#888888">[Dec 2017]</font></i>&nbsp;&nbsp; I will give a plenary talk at <a href=http://ncvpripg.iitmandi.ac.in/plenary-speakers.html>NCVPRIPG 2017</a>, one of the two major national conferences on visual computing.
</li>

<li>
  <i><font color="#888888">[Nov 2017]</font></i>&nbsp;&nbsp; The ACM selected <a href=index.html#sigasia2017>ComplementMe</a>, our label-free part suggestion technique, to feature in a <a href=https://www.eurekalert.org/pub_releases/2017-11/afcm-sad120417.php>press release</a>.
</li>

<li>
  <i><font color="#888888">[Oct 2017]</font></i>&nbsp;&nbsp; I will serve on the Program Committee of <a href=http://smi2018.tecnico.ulisboa.pt/>Shape Modeling International (SMI) 2018</a>.
</li>

<li>
  <i><font color="#888888">[Aug 2017]</font></i>&nbsp;&nbsp; We posted two new preprints: on a novel neural network architecture for weakly-supervised shape segmentation; and on a deep recurrent architecture for cleaning motion capture data.
</li>

<li>
  <i><font color="#888888">[Jul 2017]</font></i>&nbsp;&nbsp; Our <a href=index.html#sigasia2017>paper</a> on a weakly-supervised neural network-based method for component suggestion was conditionally accepted to SIGGRAPH Asia 2017.
</li>

<li>
  <i><font color="#888888">[Jul 2017]</font></i>&nbsp;&nbsp; The ACM selected <a href=index.html#siggraph2017>GRASS</a>, our generative recursive autoencoder for shape structures, to feature in a <a href=https://www.eurekalert.org/pub_releases/2017-07/afcm-md3071817.php>press release</a>.
</li>

<li>
  <i><font color="#888888">[Jun 2017]</font></i>&nbsp;&nbsp; I will serve on the Program Committee of <a href=https://aaai.org/Conferences/AAAI/aaai18.php>AAAI 2018</a> (and also the Demos program).
</li>

<li>
  <i><font color="#888888">[Mar 2017]</font></i>&nbsp;&nbsp; One paper on recursive autoencoders for 3D shapes conditionally accepted to SIGGRAPH 2017, and another paper on multi-view CNN-based 3D point descriptors conditionally accepted to TOG.
</li>

<li>
  <i><font color="#888888">[Feb 2017]</font></i>&nbsp;&nbsp; Our <a href=index.html#cvpr2017>paper</a> on 3D shape segmentation with view-based CNNs was accepted to CVPR 2017. Camera-ready version with finalized stats coming soon.
</li>

<li>
  <i><font color="#888888">[Dec 2016]</font></i>&nbsp;&nbsp; We have a <a href=https://arxiv.org/abs/1612.02808>new preprint</a> on 3D shape segmentation with view-based CNNs.
</li>

<li>
  <i><font color="#888888">[Dec 2016]</font></i>&nbsp;&nbsp; I taught a <a href=http://www.iitg.ernet.in/icvgip2016/Tutorials.php>tutorial on high-level shape analysis</a> at ICVGIP '16 in IIT Guwahati. My <a href=docs/icvgip2016_tutorial.pdf>slides</a> are now online.
</li>

<li>
  <i><font color="#888888">[Jun 2016]</font></i>&nbsp;&nbsp; Sanjeev Mk won the prestigious Qualcomm Innovation Fellowship 2016 India for his ongoing work on 3D shape analysis. Four such fellowships were awarded, and Sanjeev's was the sole winning team that comprised just one Masters student. Congratulations Sanjeev!
</li>

<li>
  <i><font color="#888888">[May 2016]</font></i>&nbsp;&nbsp; Our <a href=index.html#sgp2016>paper</a> on sketch-driven extraction of customized parts from 3D shapes was accepted to SGP 2016.
</li>

<li>
  <i><font color="#888888">[Nov 2015]</font></i>&nbsp;&nbsp; Fuse, the character creator based on my PhD research for which I wrote the first version, hits the <a href=http://www.adobe.com/products/fuse.html>big time</a>.
</li>

<li>
  <i><font color="#888888">[Aug 2015]</font></i>&nbsp;&nbsp; I moved to the <a href=http://en.wikipedia.org/wiki/Mumbai>Maximum City</a>.
</li>

<li>
  <i><font color="#888888">[Apr 2015]</font></i>&nbsp;&nbsp; Our <a href=index.html#siggraph2015>paper</a> on semantic shape editing with deformation handles (a continuous-deformation counterpart to our <a href=index.html#uist2013>UIST 2013 paper</a>) was accepted to SIGGRAPH 2015.
</li>

<li>
  <i><font color="#888888">[Apr 2015]</font></i>&nbsp;&nbsp; I was selected as one of five Outstanding Faculty Members across all departments by the Cornell Class of 2018. (Thanks folks!)
</li>

<li>
  <i><font color="#888888">[Dec 2014]</font></i>&nbsp;&nbsp; I will co-teach a one-day course on <a href=http://www.kevinkaixu.net/courses/ddvc.html>"Data-Driven Visual Computing"</a> at SIGGRAPH Asia 2014 in Shenzhen, with <a href=http://geometry.stanford.edu/member/guibas/>Leo Guibas</a>, <a href=http://www.eecs.berkeley.edu/~efros/>Alyosha Efros</a>, <a href=http://cg.cs.tsinghua.edu.cn/prof_hu.htm>Shi-Min Hu</a>, <a href=http://www.faculty.idc.ac.il/arik/>Arik Shamir</a>, <a href=http://www.kevinkaixu.net>Kevin Xu</a> and <a href=http://www.eecs.berkeley.edu/~junyanz/>Jun-Yan Zhu</a>.
</li>

<li>
  <i><font color="#888888">[Nov 2014]</font></i>&nbsp;&nbsp; <a href=index.html#uist2013>AttribIt</a> was covered in Princeton's <em>Discovery</em> research magazine. "<a href=https://discovery.princeton.edu/2014/11/17/computer-science-fierce-fiercer-fiercest-software-enables-rapid-creations/>Fierce, Fiercer, Fiercest: Software enables rapid creations</a> -- A new software program makes it easy for novices to create computer-based 3-D models using simple instructions such as 'make it look scarier'."
</li>

<li>
  <i><font color="#888888">[Aug 2014]</font></i>&nbsp;&nbsp; Our <a href=index.html#sigasia2014>paper</a> on hierarchical analysis of 3D scenes was conditionally accepted to SIGGRAPH Asia 2014.
</li>

<li>
  <i><font color="#888888">[Jun 2014]</font></i>&nbsp;&nbsp; <a href=http://www.mixamo.com/fuse/>Fuse</a> 1.1 supports adjustment shapes, for freeform deformation of body parts. See the <a href=http://youtu.be/ZVBPHI4UXfE>video</a>.
</li>

<li>
  <i><font color="#888888">[Apr 2014]</font></i>&nbsp;&nbsp; Our <a href=index.html#siggraph2014>paper</a> on human-centric shape analysis was accepted to SIGGRAPH 2014.
</li>

<li>
  <i><font color="#888888">[Mar 2014]</font></i>&nbsp;&nbsp; <a href=http://www.mixamo.com/fuse/>Fuse</a> is no longer in beta! <a href=http://www.mixamo.com>Mixamo Inc.</a> released version 1.0 of the Fuse character modeling tool at <a href="http://schedule.gdconf.com/session-id/828140">GDC</a>.
</li>

<li>
  <i><font color="#888888">[Jan 2014]</font></i>&nbsp;&nbsp; I'm co-organizing the first <a href=http://shape.cs.princeton.edu/twig14>Tristate Workshop on Imaging and Graphics</a> (TWIG), March 22-23, 2014, at Princeton, co-hosted with (and featuring an extensive list of speakers from) the SIGGRAPH Program Committee meeting.
</li>

<li>
  <i><font color="#888888">[Jan 2014]</font></i>&nbsp;&nbsp; Code for the probabilistic model described in our SIGGRAPH '12 <a href=index.html#siggraph2012>paper</a> on component-based shape synthesis is available on the <a href=http://people.cs.umass.edu/~kalo/papers/ShapeSynthesis/index.html>project page</a>. It's offered for reference purposes, as is. If you find a bug in the code, please let us know.
</li>

<li>
  <i><font color="#888888">[Nov 2013]</font></i>&nbsp;&nbsp; <a href=http://www.mixamo.com>Mixamo Inc.</a> released an Early Access version of the <a href=http://www.mixamo.com/fuse/>Fuse character modeling tool</a> on the <a href=http://store.steampowered.com/app/257400>Steam store</a>. Fuse grew out of my <a href=http://purl.stanford.edu/vq766tr8762>PhD thesis</a> work, and I was the architect and chief developer. Fuse was previously <a href=http://schedule2013.gdconf.com/session-id/824179>demoed at GDC</a>.
</li>

<li>
  <i><font color="#888888">[Jul 2013]</font></i>&nbsp;&nbsp; Our <a href=index.html#uist2013>paper</a> on exploratory design with semantic attributes ("make the animal more scary", "make the airplane more military", "make the webpage more artistic") was accepted to <a href=http://www.acm.org/uist/uist2013/>UIST 2013</a>.
</li>

</ul>

<br><br>

</div>

</body>

</html>
